# ü§ó Hugging Face Inference Endpoints - Gu√≠a Completa (Noviembre 2025)

## üìã Resumen

Los **Inference Endpoints** de Hugging Face son un servicio gestionado que permite desplegar modelos de IA en producci√≥n sin preocuparse por la infraestructura. Seg√∫n la [documentaci√≥n oficial](https://huggingface.co/docs/inference-endpoints/about), este servicio maneja todo el ciclo de vida de los contenedores.

## üéØ ¬øQu√© son los Inference Endpoints?

Los Inference Endpoints traen juntos tres componentes clave:

1. **Model Weights and Artifacts**: Par√°metros entrenados almacenados en el Hugging Face Hub
2. **Inference Engine**: Software que carga y ejecuta el modelo (vLLM, TGI, SGLang, llama.cpp, TEI)
3. **Production Infrastructure**: Infraestructura escalable, segura y confiable gestionada por Hugging Face

## üöÄ Motores de Inferencia Soportados

- **vLLM**: Alto rendimiento para modelos grandes
- **Text-generation-inference (TGI)**: Optimizado para generaci√≥n de texto
- **SGLang**: Motor de inferencia eficiente
- **llama.cpp**: Para modelos Llama
- **Text-embeddings-inference (TEI)**: Para embeddings

## üìç Regiones Disponibles (Noviembre 2025)

- **AWS**: us-east-1 (N. Virginia), eu-west-1 (Irlanda)
- **Azure**: eastus (Virginia)
- **GCP**: us-east4 (Virginia)

## üîß C√≥mo Crear un Inference Endpoint

### Pasos:

1. **Acceder a Inference Endpoints**: Inicia sesi√≥n en Hugging Face y navega a la secci√≥n de Inference Endpoints
2. **Seleccionar modelo**: Elige un modelo del Hugging Face Hub
3. **Configurar instancia**: Selecciona proveedor (AWS/Azure/GCP), regi√≥n y tipo de instancia
4. **Configurar escalado**: Define m√≠nimo/m√°ximo de r√©plicas y scale-to-zero si lo deseas
5. **Definir seguridad**: Configura nivel de acceso (p√∫blico/privado)
6. **Crear endpoint**: Haz clic en "Crear Endpoint" y espera 1-5 minutos

### Configuraci√≥n Recomendada para Hackathons:

- **Modelo**: `google/flan-t5-base` o `microsoft/DialoGPT-medium` (modelos peque√±os y r√°pidos)
- **Instancia**: CPU o GPU peque√±a (para reducir costos)
- **Scale-to-zero**: Habilitado (se detiene cuando no hay uso)
- **Regi√≥n**: us-east-1 (AWS) - m√°s econ√≥mica

## üîå C√≥mo Usar el Endpoint en el C√≥digo

Una vez creado el endpoint, recibir√°s una URL √∫nica con el formato:
```
https://{endpoint-id}.{region}.inference.endpoints.huggingface.cloud
```

### Ejemplo de llamada HTTP:

```typescript
const response = await axios.post(
  'https://tu-endpoint-id.us-east-1.inference.endpoints.huggingface.cloud',
  {
    inputs: 'Tu prompt aqu√≠',
    parameters: {
      max_new_tokens: 100,
      temperature: 0.7,
    },
  },
  {
    headers: {
      'Authorization': `Bearer ${HUGGINGFACE_API_KEY}`,
      'Content-Type': 'application/json',
    },
  }
);
```

### Ejemplo con cURL:

```bash
curl https://tu-endpoint-id.us-east-1.inference.endpoints.huggingface.cloud \
  -X POST \
  -H "Authorization: Bearer $HUGGINGFACE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "inputs": "Tu prompt aqu√≠",
    "parameters": {
      "max_new_tokens": 100,
      "temperature": 0.7
    }
  }'
```

## üí∞ Pricing y Costos

**IMPORTANTE**: Los Inference Endpoints son un servicio de pago. No hay tier gratuito permanente, pero:

- Puedes habilitar **scale-to-zero** para que el endpoint se detenga cuando no hay uso
- Solo pagas por el tiempo que el endpoint est√° activo
- Para hackathons, considera usar instancias peque√±as y scale-to-zero

### Alternativas Gratuitas:

1. **API de Inference P√∫blica** (deprecada): `api-inference.huggingface.co` - Ya no funciona
2. **Router Endpoint** (limitado): `router.huggingface.co/hf-inference` - Solo algunos modelos
3. **Inference Providers**: Algunos modelos disponibles a trav√©s de partners

## üîÑ Integraci√≥n en MetaPredict

### Opci√≥n 1: Usar Inference Endpoint Dedicado (Recomendado para Producci√≥n)

Si creas un Inference Endpoint, actualiza el servicio:

```typescript
// backend/src/services/llm/huggingface.service.ts
private endpointUrl = process.env.HUGGINGFACE_ENDPOINT_URL || 
  'https://tu-endpoint-id.us-east-1.inference.endpoints.huggingface.cloud';
```

### Opci√≥n 2: Continuar sin Hugging Face (Recomendado para Hackathons)

El sistema de consenso funciona perfectamente con las otras 4 IAs:
- ‚úÖ Gemini 2.5 Flash
- ‚úÖ Groq (Llama 3.1)
- ‚úÖ OpenAI GPT-3.5 Turbo
- ‚ö†Ô∏è Hugging Face (opcional, requiere Inference Endpoint)

## üìö Referencias

- [Documentaci√≥n Oficial - About](https://huggingface.co/docs/inference-endpoints/about)
- [Gu√≠a de Creaci√≥n de Endpoints](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint)
- [Gu√≠a de Pruebas de Endpoints](https://huggingface.co/docs/inference-endpoints/guides/test_endpoint)
- [FAQ de Inference Endpoints](https://huggingface.co/docs/inference-endpoints/faq)

## ‚ö†Ô∏è Notas Importantes

1. **API P√∫blica Deprecada**: El endpoint `api-inference.huggingface.co` est√° completamente deprecado
2. **Router Endpoint Limitado**: El endpoint `router.huggingface.co/hf-inference` solo funciona con algunos modelos espec√≠ficos
3. **Inference Endpoints Requieren Pago**: No hay tier gratuito permanente, pero scale-to-zero ayuda a minimizar costos
4. **Para Hackathons**: Considera usar solo las 4 IAs que funcionan gratuitamente (Gemini, Groq, OpenAI, Anthropic)

## üéØ Recomendaci√≥n para MetaPredict

Para el hackathon actual:
- ‚úÖ Continuar con 4 IAs (Gemini, Groq, OpenAI, Anthropic)
- ‚úÖ El sistema de consenso funciona perfectamente sin Hugging Face
- ‚ö†Ô∏è Hugging Face puede agregarse m√°s tarde si se crea un Inference Endpoint dedicado

Para producci√≥n:
- Considerar crear un Inference Endpoint dedicado para Hugging Face
- Usar scale-to-zero para optimizar costos
- Monitorear el uso y ajustar la configuraci√≥n seg√∫n necesidad

